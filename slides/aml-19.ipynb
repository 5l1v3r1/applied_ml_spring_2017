{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "% matplotlib inline\n",
    "plt.rcParams[\"figure.dpi\"] = 300\n",
    "np.set_printoptions(precision=3, suppress=True)\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split, cross_val_score\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.preprocessing import scale, StandardScaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[01;34m../data/aclImdb\u001b[00m\r\n",
      "├── \u001b[01;34mtest\u001b[00m\r\n",
      "│   ├── \u001b[01;34mneg\u001b[00m\r\n",
      "│   └── \u001b[01;34mpos\u001b[00m\r\n",
      "└── \u001b[01;34mtrain\u001b[00m\r\n",
      "    ├── \u001b[01;34mneg\u001b[00m\r\n",
      "    └── \u001b[01;34mpos\u001b[00m\r\n",
      "\r\n",
      "6 directories\r\n"
     ]
    }
   ],
   "source": [
    "!tree -dL 2 ../data/aclImdb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "type of text_train: <class 'list'>\n",
      "length of text_train: 25000\n",
      "text_train[1]:\n",
      "b'Words can\\'t describe how bad this movie is. I can\\'t explain it by writing only. You have too see it for yourself to get at grip of how horrible a movie really can be. Not that I recommend you to do that. There are so many clich\\xc3\\xa9s, mistakes (and all other negative things you can imagine) here that will just make you cry. To start with the technical first, there are a LOT of mistakes regarding the airplane. I won\\'t list them here, but just mention the coloring of the plane. They didn\\'t even manage to show an airliner in the colors of a fictional airline, but instead used a 747 painted in the original Boeing livery. Very bad. The plot is stupid and has been done many times before, only much, much better. There are so many ridiculous moments here that i lost count of it really early. Also, I was on the bad guys\\' side all the time in the movie, because the good guys were so stupid. \"Executive Decision\" should without a doubt be you\\'re choice over this one, even the \"Turbulence\"-movies are better. In fact, every other movie in the world is better than this one.'\n"
     ]
    }
   ],
   "source": [
    "from sklearn.datasets import load_files\n",
    "\n",
    "reviews_train = load_files(\"../data/aclImdb/train/\")\n",
    "# load_files returns a bunch, containing training texts and training labels\n",
    "text_train, y_train = reviews_train.data, reviews_train.target\n",
    "print(\"type of text_train: {}\".format(type(text_train)))\n",
    "print(\"length of text_train: {}\".format(len(text_train)))\n",
    "print(\"text_train[1]:\\n{}\".format(text_train[1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "... than this ;-) What would happen if Terry Gilliam and Douglas Adams would have worked together on one movie? This movie starts with a touch of Brazil... when, at a certain point, the story moves straight into the twilight zone... bringing up nothing new, but just nothing... and nothing is great fun! When Dave and Andrew starts to explore their new environment the movie gets really enjoyable... bouncing heads? well... yes ;-) <br /><br />anyway... this movie was, imho, the biggest surprise at this year's FantasyFilmFest...<br /><br />Just like in Cube and Cypher Natali gave this one a minimalistic, weird but very special design, which makes it hard to locate the place of the story or its time... timeless somehow...\n"
     ]
    }
   ],
   "source": [
    "print(text_train[11451].decode())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "**May Contain Spoilers**<br /><br />A dude in a dopey-looking Kong suit (the same one used in KING KONG VS. GODZILLA in 1962) provides much of the laffs in this much-mocked monster flick. Kong is resurrected on Mondo Island and helps out the lunkhead hero and other good guys this time around. The vampire-like villain is named Dr. Who-funny, he doesn't look like Peter Cushing! Kong finally dukes it out with Who's pride and joy, a giant robot ape that looks like a bad metal sculpture of Magilla Gorilla. Like many of Honda's flicks this may have had some merit before American audiences diddled around with it and added new footage. The Rankin/Bass animation company had a hand in this mess. They should have stuck to superior children's programs like The Little Drummer Boy.\n"
     ]
    }
   ],
   "source": [
    "print(text_train[16019].decode())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import xml.etree.ElementTree as ET"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import requests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "response = requests.get(\"http://www.europarl.europa.eu/meps/en/xml.html?query=full&filter=all\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "data_xml = ET.fromstring(response.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "members_xml = data_xml.getchildren()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "members_dict = [{i.tag: i.text for i in member} for member in members_xml]\n",
    "members = pd.DataFrame(members_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>country</th>\n",
       "      <th>fullName</th>\n",
       "      <th>id</th>\n",
       "      <th>nationalPoliticalGroup</th>\n",
       "      <th>politicalGroup</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Sweden</td>\n",
       "      <td>Lars ADAKTUSSON</td>\n",
       "      <td>124990</td>\n",
       "      <td>Kristdemokraterna</td>\n",
       "      <td>Group of the European People's Party (Christia...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Italy</td>\n",
       "      <td>Isabella ADINOLFI</td>\n",
       "      <td>124831</td>\n",
       "      <td>Movimento 5 Stelle</td>\n",
       "      <td>Europe of Freedom and Direct Democracy Group</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Italy</td>\n",
       "      <td>Marco AFFRONTE</td>\n",
       "      <td>124797</td>\n",
       "      <td>Movimento 5 Stelle</td>\n",
       "      <td>Group of the Greens/European Free Alliance</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Italy</td>\n",
       "      <td>Laura AGEA</td>\n",
       "      <td>124811</td>\n",
       "      <td>Movimento 5 Stelle</td>\n",
       "      <td>Europe of Freedom and Direct Democracy Group</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>United Kingdom</td>\n",
       "      <td>John Stuart AGNEW</td>\n",
       "      <td>96897</td>\n",
       "      <td>United Kingdom Independence Party</td>\n",
       "      <td>Europe of Freedom and Direct Democracy Group</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          country           fullName      id  \\\n",
       "0          Sweden    Lars ADAKTUSSON  124990   \n",
       "1           Italy  Isabella ADINOLFI  124831   \n",
       "2           Italy     Marco AFFRONTE  124797   \n",
       "3           Italy         Laura AGEA  124811   \n",
       "4  United Kingdom  John Stuart AGNEW   96897   \n",
       "\n",
       "              nationalPoliticalGroup  \\\n",
       "0                  Kristdemokraterna   \n",
       "1                 Movimento 5 Stelle   \n",
       "2                 Movimento 5 Stelle   \n",
       "3                 Movimento 5 Stelle   \n",
       "4  United Kingdom Independence Party   \n",
       "\n",
       "                                      politicalGroup  \n",
       "0  Group of the European People's Party (Christia...  \n",
       "1       Europe of Freedom and Direct Democracy Group  \n",
       "2         Group of the Greens/European Free Alliance  \n",
       "3       Europe of Freedom and Direct Democracy Group  \n",
       "4       Europe of Freedom and Direct Democracy Group  "
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "members.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "mallory = [\"Do you want ants?\",\n",
    "           \"Because that’s how you get ants.\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['ants', 'because', 'do', 'get', 'how', 'that', 'want', 'you']\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "vect = CountVectorizer()\n",
    "vect.fit(mallory)\n",
    "print(vect.get_feature_names())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<2x8 sparse matrix of type '<class 'numpy.int64'>'\n",
       "\twith 10 stored elements in Compressed Sparse Row format>"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X = vect.transform(mallory)\n",
    "X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1, 0, 1, 0, 0, 0, 1, 1],\n",
       "       [1, 1, 0, 1, 1, 1, 0, 1]])"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X.toarray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Do you want ants?', 'Because that’s how you get ants.']\n",
      "['ants' 'do' 'want' 'you']\n",
      "['ants' 'because' 'get' 'how' 'that' 'you']\n"
     ]
    }
   ],
   "source": [
    "print(mallory)\n",
    "print(vect.inverse_transform(X)[0])\n",
    "print(vect.inverse_transform(X)[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Classification example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "type of text_train: <class 'list'>\n",
      "length of text_train: 25000\n",
      "class balance: [12500 12500]\n",
      "text_train[1]:\n",
      "b'Words can\\'t describe how bad this movie is. I can\\'t explain it by writing only. You have too see it for yourself to get at grip of how horrible a movie really can be. Not that I recommend you to do that. There are so many clich\\xc3\\xa9s, mistakes (and all other negative things you can imagine) here that will just make you cry. To start with the technical first, there are a LOT of mistakes regarding the airplane. I won\\'t list them here, but just mention the coloring of the plane. They didn\\'t even manage to show an airliner in the colors of a fictional airline, but instead used a 747 painted in the original Boeing livery. Very bad. The plot is stupid and has been done many times before, only much, much better. There are so many ridiculous moments here that i lost count of it really early. Also, I was on the bad guys\\' side all the time in the movie, because the good guys were so stupid. \"Executive Decision\" should without a doubt be you\\'re choice over this one, even the \"Turbulence\"-movies are better. In fact, every other movie in the world is better than this one.'\n"
     ]
    }
   ],
   "source": [
    "from sklearn.datasets import load_files\n",
    "reviews_train = load_files(\"../data/aclImdb/train/\")\n",
    "\n",
    "text_train, y_train = reviews_train.data, reviews_train.target\n",
    "print(\"type of text_train: {}\".format(type(text_train)))\n",
    "print(\"length of text_train: {}\".format(len(text_train)))\n",
    "print(\"class balance: {}\".format(np.bincount(y_train)))\n",
    "print(\"text_train[1]:\\n{}\".format(text_train[1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "text_train_sub, text_val, y_train_sub, y_val = train_test_split(\n",
    "    text_train, y_train, stratify=y_train, random_state=0)\n",
    "vect = CountVectorizer()\n",
    "X_train = vect.fit_transform(text_train_sub)\n",
    "X_val = vect.transform(text_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<18750x66651 sparse matrix of type '<class 'numpy.int64'>'\n",
       "\twith 2580448 stored elements in Compressed Sparse Row format>"
      ]
     },
     "execution_count": 108,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "feature_names = vect.get_feature_names()\n",
    "print(feature_names[:10])\n",
    "print(feature_names[::2000])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/andy/checkout/scikit-learn/sklearn/linear_model/sag.py:286: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  \"the coef_ did not converge\", ConvergenceWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 loop, best of 3: 1min 9s per loop\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import LogisticRegressionCV\n",
    "lr = LogisticRegressionCV(solver=\"sag\").fit(X_train, y_train_sub)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 loop, best of 3: 43.2 s per loop\n"
     ]
    }
   ],
   "source": [
    "%timeit lr = LogisticRegressionCV().fit(X_train, y_train_sub)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%timeit lr = LogisticRegressionCV(solver='liblinear').fit(X_train, y_train_sub)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Vectorization options"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(?u)\\b\\w\\w+\\b\n"
     ]
    }
   ],
   "source": [
    "print(vect.token_pattern)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['ants', 'because', 'do', 'get', 'how', 's', 'that', 'want', 'you']\n"
     ]
    }
   ],
   "source": [
    "vect = CountVectorizer(token_pattern=r\"\\b\\w+\\b\")\n",
    "vect.fit(mallory)\n",
    "print(vect.get_feature_names())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['ants', 'because', 'do', 'get', 'how', 'that’s', 'want', 'you']\n"
     ]
    }
   ],
   "source": [
    "vect = CountVectorizer(token_pattern=r\"\\b\\w[\\w’]+\\b\")\n",
    "# not actually an apostroph but some unicode pattern\n",
    "# because I copy & pasted the quote\n",
    "vect.fit(mallory)\n",
    "print(vect.get_feature_names())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['ants', 'want']\n"
     ]
    }
   ],
   "source": [
    "vect = CountVectorizer(stop_words='english')\n",
    "vect.fit(mallory)\n",
    "print(vect.get_feature_names())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['there', 'else', 'two', 'perhaps', 'get', 'inc', 'find', 'interest', 'between', 'give', 'amongst', 'however', 'former', 'nine', 'please', 'us', 'about', 'almost', 'but', 'thereupon', 'call', 'ie', 'third', 'whereby', 'whole', 'whose', 'one', 'afterwards', 'only', 'somehow', 'is', 'eight', 'nothing', 'an', 'with', 'describe', 'than', 'itself', 'do', 'thin', 'cry', 'hundred', 'its', 'latterly', 'formerly', 'name', 'no', 'via', 'hereupon', 'well', 'system', 'so', 'un', 'mill', 'neither', 'she', 'seems', 'or', 'though', 'against', 'wherever', 'very', 'within', 'con', 'during', 'whom', 'per', 'front', 'much', 'sometimes', 'ten', 'next', 'those', 'anyhow', 'fill', 'became', 'along', 'never', 'this', 'that', 'our', 'all', 'be', 'may', 'made', 'should', 'for', 'keep', 'onto', 'below', 'here', 'been', 'of', 'once', 'themselves', 'whereas', 'three', 'hereby', 'several', 'how', 'even', 'whither', 'her', 'herself', 'other', 'will', 'around', 'a', 'seem', 'because', 'it', 'across', 'take', 'enough', 'to', 'under', 'what', 'again', 'less', 'through', 'amoungst', 'the', 'more', 'my', 'either', 'see', 'sometime', 'detail', 'thereafter', 'anyone', 'except', 'co', 'from', 'now', 'own', 'de', 'them', 'anywhere', 'hasnt', 'nobody', 'few', 'and', 'hence', 'alone', 'when', 'each', 'another', 'always', 'anything', 'yet', 'four', 'therefore', 'thick', 'cant', 'since', 'can', 'twelve', 'forty', 'among', 'over', 'where', 'your', 'cannot', 'on', 'becomes', 'sixty', 'whether', 'become', 'such', 'we', 'some', 'in', 'thus', 'upon', 'their', 'fifteen', 'they', 'could', 'mostly', 'was', 'although', 'serious', 'first', 'not', 'thence', 'thru', 'whenever', 'rather', 'before', 'moreover', 'noone', 'put', 'up', 'who', 'were', 'anyway', 'namely', 'beyond', 'latter', 'everyone', 'toward', 'seeming', 'whereupon', 'yourselves', 'move', 'why', 'part', 'same', 'without', 'every', 'bill', 'might', 'most', 'fifty', 'hereafter', 'show', 'have', 'herein', 'beforehand', 'off', 'which', 'indeed', 'many', 'whereafter', 'none', 'after', 'ours', 'down', 'couldnt', 'bottom', 'go', 'due', 'sincere', 'himself', 'done', 'back', 'am', 'hers', 'etc', 'last', 'out', 'six', 'nor', 'until', 'meanwhile', 'nowhere', 'twenty', 'whoever', 'must', 'full', 'whatever', 'you', 'both', 'top', 'also', 'being', 'into', 'these', 'by', 'nevertheless', 'least', 'besides', 'as', 'would', 'still', 'amount', 'behind', 'side', 'has', 'any', 'ever', 'ltd', 'together', 'ourselves', 'mine', 'wherein', 'above', 'somewhere', 'beside', 'thereby', 'had', 'i', 'myself', 'otherwise', 'whence', 'at', 'elsewhere', 'if', 'further', 'he', 'his', 'eleven', 'him', 'are', 'then', 'while', 'eg', 'often', 'already', 'too', 'yourself', 'someone', 'fire', 'found', 'others', 'me', 're', 'everything', 'something', 'therein', 'throughout', 'becoming', 'everywhere', 'yours', 'towards', 'empty', 'seemed', 'five']\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import ENGLISH_STOP_WORDS\n",
    "print(list(ENGLISH_STOP_WORDS))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['ants', 'you']\n"
     ]
    }
   ],
   "source": [
    "vect = CountVectorizer(min_df=2)\n",
    "vect.fit(mallory)\n",
    "print(vect.get_feature_names())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['ants', 'because', 'do', 'you']\n"
     ]
    }
   ],
   "source": [
    "vect = CountVectorizer(max_features=4)\n",
    "vect.fit(mallory)\n",
    "print(vect.get_feature_names())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "vect = CountVectorizer(min_df=2)\n",
    "X_train_df2 = vect.fit_transform(text_train_sub)\n",
    "X_val_df2.shape = vect.transform(text_val)\n",
    "print(X_train.shape)\n",
    "print(X_train_df2.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# n-grams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "cv = CountVectorizer(ngram_range=(1, 1)).fit(mallory)\n",
    "print(\"Vocabulary size: {}\".format(len(cv.vocabulary_)))\n",
    "print(\"Vocabulary:\\n{}\".format(cv.get_feature_names()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "cv = CountVectorizer(ngram_range=(2, 2)).fit(mallory)\n",
    "print(\"Vocabulary size: {}\".format(len(cv.vocabulary_)))\n",
    "print(\"Vocabulary:\\n{}\".format(cv.get_feature_names()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "cv = CountVectorizer(ngram_range=(1, 2)).fit(mallory)\n",
    "print(\"Vocabulary size: {}\".format(len(cv.vocabulary_)))\n",
    "print(\"Vocabulary:\\n{}\".format(cv.get_feature_names()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Character n-grams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "cv = CountVectorizer(ngram_range=(2, 3), analyzer=\"charwb\").fit(mallory)\n",
    "print(\"Vocabulary size: {}\".format(len(cv.vocabulary_)))\n",
    "print(\"Vocabulary:\\n{}\".format(cv.get_feature_names()))"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda root]",
   "language": "python",
   "name": "conda-root-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
